{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CMAQ-Ready File from Shapefile\n",
    "\n",
    "---\n",
    "    author: Barron H. Henderson\n",
    "    date: 2020-04-25\n",
    "    updated: 2020-04-29\n",
    "---\n",
    "\n",
    "This Notebook uses geopandas and PseudoNetCDF to create IOAPI-like files for CMAQ. Geopandas supports STRtree optimized searches and projection conversions. This simplifies the process to basic steps:\n",
    "\n",
    "1. Define grid cells as GeoPandas.GeoDataframe\n",
    "2. Process shapefile\n",
    "    * Read in native projection.\n",
    "    * Clip shapefile to grid\n",
    "    * Optionally, custom extra processing\n",
    "3. Perform grid cell intersections with shapefile polygons.\n",
    "4. Output\n",
    "    * Aggregate results to grid cell level\n",
    "    * Find largest area contirbutor\n",
    "    * Store results as variables\n",
    "    * Tidy metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inputs\n",
    "\n",
    "* Each input is required\n",
    "* Update according to the documentation.\n",
    "* Most users will not make edits anywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shppath : str\n",
    "#     Path to a shapefile or zip file containing a shapefile\n",
    "#     wget http://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\n",
    "#     Use 10m for better resolution\n",
    "shppath = 'ne_110m_admin_0_countries.zip'\n",
    "\n",
    "# attrkey : str\n",
    "#     Attribute of shapes in shapefile that defines groups of shapes. For\n",
    "#     exmaple, ADM0_A3 defines countries in NaturalEarth's countries file.\n",
    "attrkey = 'ADM0_A3'\n",
    "\n",
    "# gdpath : str\n",
    "#     Path to a GRIDDESC file\n",
    "gdpath = 'EXAMPLE_GRIDDESC'\n",
    "\n",
    "# gdnam : str\n",
    "#     Name of grid definition within gdpath (e.g., 12US1, 108NHEMI2)\n",
    "gdnam = '108US1'\n",
    "\n",
    "# outdir : str\n",
    "#     Directory to make outputs with file names like shppath.gdnam.IOAPI.nc\n",
    "outdir = '.'\n",
    "\n",
    "# outformat : str\n",
    "#     Use eitehr NETCDF4_CLASSIC or NETCDF3_CLASSIC. \n",
    "#    'NETCDF3_CLASSIC' will make bigger files, but more compatible with older IOAPI\n",
    "outformat = 'NETCDF4_CLASSIC'\n",
    "\n",
    "# debug : bool\n",
    "#     Plot intermediate and final results for understanding what happened.\n",
    "debug = False # if true, plot result\n",
    "\n",
    "# overwrite : bool\n",
    "#     Default False, old results will not be overwritten\n",
    "#     If True, rewrite results\n",
    "overwrite = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, box\n",
    "import pyproj\n",
    "import pycno\n",
    "import PseudoNetCDF as pnc\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootname = os.path.splitext(os.path.basename(shppath))[0]\n",
    "outpath = f'{outdir}/{rootname}.{gdnam}.IOAPI.nc'\n",
    "os.environ['IOAPI_ISPH'] = '6370000.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ./ne_110m_admin_0_countries.108US1.IOAPI.nc\n",
      "author: shapefile2cmaq\n",
      "description: ADM0_A3 fractional area coverage and dominnant (DOM_ADM0_A3)\n",
      "inputs:\n",
      " - Shapefile: ne_110m_admin_0_countries.zip\n",
      " - Attribute: ADM0_A3\n",
      " - GRIDDESC: EXAMPLE_GRIDDESC\n",
      " - GDNAM: 108US1\n",
      "tool_version: 1.0\n",
      "file_version: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FILEDESC = f\"\"\"title: {outpath}\n",
    "author: shapefile2cmaq\n",
    "description: {attrkey} fractional area coverage and dominnant (DOM_{attrkey})\n",
    "inputs:\n",
    " - Shapefile: {shppath}\n",
    " - Attribute: {attrkey}\n",
    " - GRIDDESC: {gdpath}\n",
    " - GDNAM: {gdnam}\n",
    "tool_version: 1.0\n",
    "file_version: 1.0\n",
    "\"\"\"\n",
    "print(FILEDESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(outpath):\n",
    "    if overwrite:\n",
    "        os.remove(outpath)\n",
    "    else:\n",
    "        raise IOError(f'{outpath} already exists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gdpath == \"EXAMPLE_GRIDDESC\" and not os.path.exists(gdpath):\n",
    "    with open(gdpath, 'w') as gdf:\n",
    "        gdf.write(\"\"\"' '\n",
    "'LamCon_40N_97W'\n",
    " 2         33.000        45.000       -97.000       -97.000        40.000\n",
    "' '\n",
    "'108US1'\n",
    "'LamCon_40N_97W'  -2952000.000  -2772000.000    108000.000    108000.000  60   50   1\n",
    "' '\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = pnc.pncopen(gdpath, format='griddesc', GDNAM=gdnam, SDATE=1970001)\n",
    "proj = gf.getproj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.8 ms, sys: 0 ns, total: 37.8 ms\n",
      "Wall time: 41.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gridcells = []\n",
    "\n",
    "I, J = np.meshgrid(np.arange(gf.NCOLS), np.arange(gf.NROWS))\n",
    "hdx = gf.XCELL / 2\n",
    "hdy = gf.YCELL / 2\n",
    "COL = I * gf.XCELL + hdx\n",
    "ROW = J * gf.YCELL + hdy\n",
    "\n",
    "for (ri, ci), c in np.ndenumerate(COL):\n",
    "    r = ROW[ri, ci]\n",
    "    gridcells.append(box(c - hdx, r - hdy, c + hdx, r + hdx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(dict(\n",
    "    I=I.ravel(), J=J.ravel()\n",
    "), geometry=gridcells, crs=proj.srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in native projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = gpd.read_file(shppath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, derive an attribute. For example, here I transform UTC time zone offsets\n",
    "# to create a variable whose values have no symbols\n",
    "# cdf[attrkey] = cdf['utc_format'].astype(str).str.replace('Â±', '+').str.replace('UTC', '')\n",
    "# cdf[attrkey] = cdf[attrkey].str.replace('+', 'POS_').str.replace('-', 'NEG_').str.replace(':', '')\n",
    "# cdf[attrkey].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attrkey not in cdf.columns:\n",
    "    raise KeyError(f'{attrkey} is not found in {shppath}; try {cdf.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip to grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gdf.crs.to_cf()['grid_mapping_name'] == 'polar_stereographic':\n",
    "    # polar stereographic polygons have problems in WGS84, but their centroids \n",
    "    # do not use centroids to create a point then buffer the points to create a\n",
    "    # polygon then clipping to spherical space\n",
    "    #\n",
    "    # Heuristic assumption that the buffer needs to be in degrees\n",
    "    # and the midlatitude degrees/m is 100km. Tripling for safety.\n",
    "    buff = max(gf.XCELL, gf.YCELL) / 100e3 * 3\n",
    "    centroids = gdf.geometry.centroid.to_crs(cdf.crs)\n",
    "    swx, swy, nex, ney = centroids.total_bounds\n",
    "    sedge = swy - buff\n",
    "    # Block warning about buffering in lat/lon space. Since this is\n",
    "    # very much intended, I am  hiding the warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        blobs = centroids.buffer(buff)\n",
    "\n",
    "    mask = gpd.clip(\n",
    "        blobs,\n",
    "        gpd.GeoSeries(box(-180, sedge, 180, 90), crs=4326).to_crs(cdf.crs)\n",
    "    ).unary_union\n",
    "else:\n",
    "    tmpdf = gdf.to_crs(cdf.crs)\n",
    "    mask = tmpdf[tmpdf.geometry.is_valid].geometry.unary_union.buffer(1)\n",
    "\n",
    "# Subset the spatial domain of the shapefile\n",
    "vcdf = gpd.clip(cdf, mask).to_crs(gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    gpd.GeoSeries(mask).plot()\n",
    "    vcdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra processing?\n",
    "\n",
    "* Here you could add \"extra processing\"\n",
    "* For example, surfzone can be created from coastlines.\n",
    "  * Coastline geometries would be lines instead of polygons.\n",
    "  * To get the surfzone polygon, you would add a 50m buffer.\n",
    "  * That would be a symmetric buffer, but only the ocean side is surfzone.\n",
    "  * So, you would then clip that to the ocean.\n",
    "* The result would be a polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocnpath = '/work/ROMO/gis_rasters/NaturalEarth/downloads/ne_50m_ocean.zip'\n",
    "# oceandf = gpd.clip(gpd.read_file(ocnpath), mask).to_crs(gdf.crs)\n",
    "# vcdf = gpd.clip(vcdf.buffer(50), oceandf.buffer(0))\n",
    "# vcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ne_50m_ocean.zip, consider spliting into pieces\n",
    "# vcdf = vcdf.explode().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find intersection between grid cells and polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.9 ms, sys: 2.76 ms, total: 66.7 ms\n",
      "Wall time: 66.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "intx = gpd.sjoin(gdf, vcdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 208 ms, sys: 20.5 ms, total: 228 ms\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "intx['right_geometry'] = vcdf.geometry.loc[intx.index_right].values\n",
    "if not intx['right_geometry'].is_valid.all():\n",
    "    intx['right_geometry'] = intx['right_geometry'].buffer(0)\n",
    "\n",
    "intx['intersection_geometry'] = intx['geometry'].intersection(\n",
    "    intx['right_geometry'], align=False\n",
    ")\n",
    "intx['intersection_area'] = (\n",
    "    intx['intersection_geometry'].area / gf.XCELL / gf.YCELL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate results to the GRID CELL level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage By A ADM0_A3: (3.6761891184579765e-06, 1.0)\n"
     ]
    }
   ],
   "source": [
    "attr_area = intx[\n",
    "    [attrkey, 'I', 'J', 'intersection_area']\n",
    "].groupby([attrkey, 'I', 'J', ], as_index=False).sum()\n",
    "# Limit fraction area to 1.\n",
    "attrkeys = list(attr_area[attrkey].unique())\n",
    "outkeys = sorted(attrkeys)\n",
    "attr_range = tuple(attr_area[\"intersection_area\"].describe()[[\"min\", \"max\"]])\n",
    "print(f'Coverage By A {attrkey}: {attr_range}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage By All ADM0_A3: (1.223901484760318e-05, 1.0000000000000002)\n"
     ]
    }
   ],
   "source": [
    "totkey = f'TOT_{attrkey}'\n",
    "total_area = attr_area.groupby(['I', 'J'], as_index=False).sum()\n",
    "total_range = tuple(total_area[\"intersection_area\"].describe()[[\"min\", \"max\"]])\n",
    "print(f'Coverage By All {attrkey}: {total_range}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the largest area contributor (aka dominant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "domkey = f'DOM_{attrkey}'\n",
    "dom_area = attr_area.sort_values(\n",
    "    by='intersection_area', ascending=False\n",
    ").groupby(['I', 'J'], as_index=False).head(1)\n",
    "dom_area[domkey] = dom_area[attrkey].map(attrkeys.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store results as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BHS.BLZ.CAN.COL.CUB.DOM.GTM.HND.HTI.JAM.MEX.NIC.PRI.USA.VEN.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for key in outkeys:\n",
    "    areas = attr_area.query(f'{attrkey} == \"{key}\"')\n",
    "    print(key, end='.')\n",
    "    outvar = gf.copyVariable(gf.variables['DUMMY'], key=key, dtype='f')\n",
    "    outvar.setncatts(dict(\n",
    "        long_name=key.ljust(16), var_desc=key.ljust(80), units='1'.ljust(16)\n",
    "    ))\n",
    "    outvar[:] = 0.0\n",
    "    outvar[0, 0, areas.J.values, areas.I.values] = areas.intersection_area.values\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(outkeys) > 1:\n",
    "    outvar = gf.copyVariable(gf.variables['DUMMY'], key=totkey, dtype='f')\n",
    "    outvar.setncatts(dict(\n",
    "        long_name=totkey.ljust(16), var_desc=totkey.ljust(80),\n",
    "        units='1'.ljust(16)\n",
    "    ))\n",
    "    outvar[:] = 0\n",
    "    outvar.var_desc = f'Total {attrkey} coverage'\n",
    "    outvar[0, 0, total_area.J.values, total_area.I.values] = (\n",
    "        total_area['intersection_area'].values\n",
    "    )\n",
    "    \n",
    "    outvar = gf.copyVariable(gf.variables['DUMMY'], key=domkey, dtype='i')\n",
    "    outvar.setncatts(dict(\n",
    "        long_name=domkey.ljust(16), var_desc=domkey.ljust(80),\n",
    "        units='1'.ljust(16)\n",
    "    ))\n",
    "    outvar[:] = -9999\n",
    "    outvar[0, 0, dom_area.J.values, dom_area.I.values] = (\n",
    "        dom_area[domkey].values\n",
    "    )\n",
    "    outvar.var_desc = f'Dominant {attrkey} ID see description'\n",
    "    outvar.description = f'{dict(enumerate(attrkeys))}'\n",
    "    outkeys.append(totkey)\n",
    "    outkeys.append(domkey)\n",
    "    FILEDESC = FILEDESC + f'{domkey}: {outvar.description}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.SDATE = 1970001\n",
    "outf = gf.subset(outkeys)\n",
    "outf.FILEDESC = FILEDESC.ljust(80*60)[:80*60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamers = {key: key[:15] for key in outf.variables if len(key) >= 16}\n",
    "if len(renamers) > 0:\n",
    "    outf = outf.renameVariables(**renamers)\n",
    "    outkeys = [renamers.get(key, key) for key in outkeys]\n",
    "# Optionally, rename variables for IOAPI compliance\n",
    "# outf = outf.renameVariables(\n",
    "#     'REALLLLLLLLY_TOO_LONG_NAME': 'LONG_NAME'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    plt.colorbar(plt.pcolormesh(\n",
    "        np.ma.masked_less(outf.variables[outkeys[-1]][0, 0], -1000)\n",
    "    ))\n",
    "    pycno.cno(proj=outf.getproj(withgrid=True)).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "outf.variables['TFLAG'][:] = 0\n",
    "outf.save(outpath, complevel=1, format=outformat, verbose=0).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
